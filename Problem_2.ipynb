{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Getting familiar with TensorFlow\n",
    "\n",
    "*TensorFlow* is one of the most popular deep learning framework developed by Google. If you are new to TensorFlow, please read and play with the sample in [Getting started with TensorFlow](https://www.tensorflow.org/get_started/get_started) to get started.\n",
    "\n",
    "* <b>Learning Objective:</b> In Problem 1, you implemented a fully connected network from scratch on your own. Very tedious to do it all by yourself, right? Well, we actually feel the same thing, that's why we are using tools instead of doing everything from scratch. For this part of the assignment, we will familiarize you with a widely-used deep learning framework developed by Google, TensorFlow and walk you through convolutional neural networks and show how to train them.\n",
    "* <b>Provided Codes:</b> We provide the Template class for a simple CNN model as BaseModel, predefined skeletons for conv2d() and max_pool(), as well as the dataset preprocessing parts.\n",
    "* <b>TODOs:</b> You are asked to implement the BaseModel following the detailed instructions and design your own model in YourModel to achieve a reasonably good performance for classification task on CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 1.14.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "# Add whatever you want\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from lib.datasets import CIFAR10_tf\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# We recommend to use tensorflow==1.14.0\n",
    "print(\"TensorFlow Version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "Download [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz) and load the dataset. In this assignment, we will use the standard 50,000 images for training and 10,000 images for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "num_training = 49000\n",
    "num_validation = 50000 - num_training\n",
    "num_test = 10000\n",
    "\n",
    "data = CIFAR10_tf(num_training=num_training,\n",
    "                  num_validation=num_validation,\n",
    "                  num_test=num_test)\n",
    "\n",
    "# Load cifar-10 data\n",
    "X_train, Y_train = data['data_train'], data['labels_train']\n",
    "X_val, Y_val = data['data_val'], data['labels_val']\n",
    "X_test, Y_test = data['data_test'], data['labels_test']\n",
    "\n",
    "# Check the shape of the dataset\n",
    "assert X_train.shape == (num_training, 32, 32, 3)\n",
    "assert Y_train.shape == (num_training, )\n",
    "assert X_val.shape == (num_validation, 32, 32, 3)\n",
    "assert Y_val.shape == (num_validation, )\n",
    "assert X_test.shape == (num_test, 32, 32, 3)\n",
    "assert Y_test.shape == (10000, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train_count: Counter({4: 4922, 2: 4921, 0: 4913, 5: 4902, 6: 4898, 9: 4895, 7: 4893, 3: 4888, 8: 4887, 1: 4881})\n",
      "Y_val_count: Counter({1: 119, 8: 113, 3: 112, 7: 107, 9: 105, 6: 102, 5: 98, 0: 87, 2: 79, 4: 78})\n",
      "Y_test_count: Counter({3: 1000, 8: 1000, 0: 1000, 6: 1000, 1: 1000, 9: 1000, 5: 1000, 7: 1000, 4: 1000, 2: 1000})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Y_train_count = Counter(Y_train)\n",
    "Y_val_count = Counter(Y_val)\n",
    "Y_test_count = Counter(Y_test)\n",
    "\n",
    "print(f'Y_train_count: {Y_train_count}')\n",
    "print(f'Y_val_count: {Y_val_count}')\n",
    "print(f'Y_test_count: {Y_test_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2-1 [10pt]\n",
    "\n",
    "Using the code provided, implement a neural network architecture with an optimization routine according to the specification provided below.\n",
    "\n",
    "**Model:**\n",
    "- Input image with the size 32x32x3\n",
    "- 7x7 convolutional layer with 32 filters, stride of 1, and padding 'SAME'\n",
    "- ReLU activation layer\n",
    "- 3x3 max pooling layer with a stride of 2\n",
    "- 5x5 convolutional layer with 64 filters, stride of 1, and padding 'SAME'\n",
    "- ReLU activation layer\n",
    "- 3x3 max pooling layer with a stride of 2\n",
    "- Flatten layer (8x8x64 -> 4096)\n",
    "- Fully-connected layer with 384 output units (4096 -> 384)\n",
    "- ReLU activation layer\n",
    "- Fully-connected layer with 10 output units (384 -> 10)\n",
    "- Output logits (10)\n",
    "\n",
    "**Optimizer:**\n",
    "- Adam optimizer\n",
    "\n",
    "**Learning rate:**\n",
    "- Set start learning rate as 5e-4 and apply exponential decay every 500 steps with a base of 0.96\n",
    "- Use 'tf.train.exponential_decay' and 'tf.train.AdamOptimizer'\n",
    "\n",
    "**Loss:**\n",
    "- Softmax cross entropy loss\n",
    "- Use 'tf.nn.softmax_cross_entropy_with_logits_v2'\n",
    "\n",
    "\n",
    "Your model **should** achieve about 55% accuracy on test set in 5 epochs using provided evaluation code.\n",
    "\n",
    "You can modify the template code as you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max pooling and conv layers\n",
    "\n",
    "def conv2d(input, kernel_size, stride, num_filter):\n",
    "    stride_shape = [1, stride, stride, 1]\n",
    "    filter_shape = [kernel_size, kernel_size, input.get_shape()[3], num_filter]\n",
    "    W = tf.get_variable('w', filter_shape, tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "    b = tf.get_variable('b', [1, 1, 1, num_filter], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.nn.conv2d(input, W, stride_shape, padding='SAME') + b\n",
    "\n",
    "def max_pool(input, kernel_size, stride):\n",
    "    ksize = [1, kernel_size, kernel_size, 1]\n",
    "    strides = [1, stride, stride, 1]\n",
    "    return tf.nn.max_pool(input, ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Complete the following functions                                    #\n",
    "#############################################################################\n",
    "def flatten(input):\n",
    "    \"\"\"\n",
    "        - input: input tensors\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.layers.flatten(input)\n",
    "\n",
    "def fc(input, num_output):\n",
    "    \"\"\"\n",
    "        - input: input tensors\n",
    "        - num_output: int, the output dimension\n",
    "    \"\"\"\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale = 0.0001)\n",
    "    return tf.contrib.layers.fully_connected(input, num_output, activation_fn = None, weights_regularizer = regularizer)\n",
    "\n",
    "def conv2d_regularized(input, kernel_size, stride, num_filter):\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale = 0.0001)\n",
    "    return tf.layers.conv2d(input, num_filter, kernel_size, stride, kernel_initializer = tf.random_normal_initializer(0.0, 0.02), kernel_regularizer = regularizer, padding='same')\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    def __init__(self):\n",
    "        self.num_epoch = 5\n",
    "        self.batch_size = 128\n",
    "        self.log_step = 100\n",
    "        self._build_model()\n",
    "\n",
    "    def _model(self):\n",
    "        print('-' * 5 + '  Sample model  ' + '-' * 5)\n",
    "\n",
    "        print('intput layer: ' + str(self.X.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv1'):\n",
    "            self.conv1 = conv2d(self.X, 7, 1, 32)\n",
    "            self.relu1 = tf.nn.relu(self.conv1)\n",
    "            self.pool1 = max_pool(self.relu1, 3, 2)            \n",
    "            print('conv1 layer: ' + str(self.pool1.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv2'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.conv2 = conv2d(self.pool1, 5, 1, 64)\n",
    "            self.relu2 = tf.nn.relu(self.conv2)\n",
    "            self.pool2 = max_pool(self.relu2, 3, 2)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('conv2 layer: ' + str(self.pool2.get_shape()))\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: Flatten the output tensor from conv2 layer                          #\n",
    "        #############################################################################\n",
    "        self.flat = flatten(self.pool2)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################      \n",
    "        print('flat layer: ' + str(self.flat.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc3'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.fc3 = fc(self.flat, 384)\n",
    "            self.relu3 = tf.nn.relu(self.fc3)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('fc3 layer: ' + str(self.relu3.get_shape()))\n",
    "            \n",
    "        with tf.variable_scope('fc4'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.fc4 = fc(self.relu3, 10)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('fc4 layer: ' + str(self.fc4.get_shape()))\n",
    "            \n",
    "        # Return the last layer\n",
    "        return self.fc4\n",
    "\n",
    "    def _input_ops(self):\n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: You can add any placeholders                                        #\n",
    "        #############################################################################\n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        #############################################################################\n",
    "        # TODO: Adam optimizer 'self.train_op' that minimizes 'self.loss_op'        #\n",
    "        #############################################################################\n",
    "        self.step = tf.get_variable(name = 'step', dtype = tf.int32, initializer = 0)\n",
    "        self.rate = tf.train.exponential_decay(5e-4, self.step, 500, 0.96)\n",
    "        self.train_op = tf.train.AdamOptimizer(self.rate).minimize(self.loss_op, global_step=self.step)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        \n",
    "    def _loss(self, labels, logits):\n",
    "        #############################################################################\n",
    "        # TODO: Softmax cross entropy loss 'self.loss_op'                           #\n",
    "        #############################################################################\n",
    "        self.loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels, logits, name='loss'))\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Define input variables\n",
    "        self._input_ops()\n",
    "\n",
    "        # Convert Y to one-hot vector\n",
    "        labels = tf.one_hot(self.Y, 10)\n",
    "\n",
    "        # Build a model and get logits\n",
    "        logits = self._model()\n",
    "\n",
    "        # Compute loss\n",
    "        self._loss(labels, logits)\n",
    "        \n",
    "        # Build optimizer\n",
    "        self._build_optimizer()\n",
    "\n",
    "        # Compute accuracy\n",
    "        predict = tf.argmax(logits, 1)\n",
    "        correct = tf.equal(predict, self.Y)\n",
    "        self.accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "    def train(self, sess, X_train, Y_train, X_val, Y_val):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        print('-' * 5 + '  Start training  ' + '-' * 5)\n",
    "        for epoch in range(self.num_epoch):\n",
    "            print('train for epoch %d' % epoch)\n",
    "            for i in range(num_training // self.batch_size):\n",
    "                X_ = X_train[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "                Y_ = Y_train[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "                #############################################################################\n",
    "                # TODO: You can change feed data as you want                                #\n",
    "                #############################################################################\n",
    "                feed_dict = {self.X: X_, self.Y: Y_}\n",
    "                #############################################################################\n",
    "                #                             END OF YOUR CODE                              #\n",
    "                #############################################################################\n",
    "                fetches = [self.train_op, self.loss_op, self.accuracy_op]\n",
    "\n",
    "                _, loss, accuracy = sess.run(fetches, feed_dict=feed_dict)\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "\n",
    "                if step % self.log_step == 0:\n",
    "                    print('iteration (%d): loss = %.3f, accuracy = %.3f' %\n",
    "                        (step, loss, accuracy))\n",
    "                step += 1\n",
    "\n",
    "            # Print validation results\n",
    "            print('validation for epoch %d' % epoch)\n",
    "            val_accuracy = self.evaluate(sess, X_val, Y_val)\n",
    "            print('-  epoch %d: validation accuracy = %.3f' % (epoch, val_accuracy))\n",
    "            \n",
    "        #############################################################################\n",
    "        # TODO: Plot training curve                                                 #\n",
    "        #############################################################################\n",
    "        # Graph 1. X: iteration (training step), Y: training loss\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.title('Training loss vs Iterations')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot(losses, '-o')\n",
    "        # Graph 2. X: iteration (training step), Y: training accuracy\n",
    "        \n",
    "        plt.subplot(2,1,2)\n",
    "        plt.title('Training accuracy vs Iterations')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.plot(accuracies, '-o')\n",
    "        \n",
    "        plt.gcf().set_size_inches(15, 15)\n",
    "        plt.show()\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def evaluate(self, sess, X_eval, Y_eval):\n",
    "        eval_accuracy = 0.0\n",
    "        eval_iter = 0\n",
    "        for i in range(X_eval.shape[0] // self.batch_size):\n",
    "            X_ = X_eval[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "            Y_ = Y_eval[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            \n",
    "            #############################################################################\n",
    "            # TODO: You can change feed data as you want                                #\n",
    "            #############################################################################\n",
    "            feed_dict = {self.X: X_, self.Y: Y_}\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            accuracy = sess.run(self.accuracy_op, feed_dict=feed_dict)\n",
    "            eval_accuracy += accuracy\n",
    "            eval_iter += 1\n",
    "        return eval_accuracy / eval_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Clear old computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Train our sample model\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    with tf.device('/cpu:0'):\n",
    "        model = BaseModel()\n",
    "        model.train(sess, X_train, Y_train, X_val, Y_val)\n",
    "        accuracy = model.evaluate(sess, X_test, Y_test)\n",
    "        print('***** test accuracy: %.3f' % accuracy)\n",
    "        saver = tf.train.Saver()\n",
    "        model_path = saver.save(sess, \"lib/tf_models/problem2/csci-599_sample.ckpt\")\n",
    "        print(\"Model saved in %s\" % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2-2 [15pt]\n",
    "\n",
    "Implement your own model. \n",
    "\n",
    "You can modify the template code as you want and you can use GPU for fast training. For GPU usage, simply change the following line of the training block:  \n",
    "from `with tf.device('/cpu:0')` to `with tf.device('/GPU:0')` and you can set your desired device number.\n",
    "\n",
    "These are the techniques that you can try:\n",
    "- Data preprocessing\n",
    "- Data augmentation\n",
    "- Batch normalization\n",
    "- Dropout\n",
    "- More convolutional layers\n",
    "- More training epochs\n",
    "- Learning rate decay\n",
    "- Any other models and techniqes\n",
    "\n",
    "The rubrics for this question is:\n",
    "* 15 points when test accuracy >= 75%\n",
    "* 10 points when test accuracy >= 70%\n",
    "* 5 points when test accuracy >= 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class YourModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        super(YourModel, self).__init__()\n",
    "        self.num_epoch = 50\n",
    "        self.principal_components = None\n",
    "        self.zca_epsilon = 1e-6\n",
    "    \n",
    "    def _input_ops(self):\n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "        self.training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        \n",
    "    def _loss(self, labels, logits):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels, logits, name='vanilla_loss'))\n",
    "        l2_regularizer_loss = tf.losses.get_regularization_loss()\n",
    "        self.reg_loss_op = l2_regularizer_loss\n",
    "        self.loss_op = loss + l2_regularizer_loss\n",
    "\n",
    "    def _model(self):\n",
    "        print('-' * 5 + '  Your model  ' + '-' * 5)\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: Implement you own model here                                        #\n",
    "        #############################################################################\n",
    "        self.keep_prob = 0.5\n",
    "        print('intput layer: ' + str(self.X.get_shape()))\n",
    "        \n",
    "        with tf.variable_scope('conv1'):\n",
    "            self.conv1 = conv2d_regularized(self.X, 7, 1, 32)\n",
    "            self.bn1 = tf.layers.batch_normalization(self.conv1, training = self.training)\n",
    "            self.elu1 = tf.nn.elu(self.bn1)\n",
    "            self.do1 = tf.nn.dropout(self.elu1, keep_prob = self.keep_prob)\n",
    "            self.pool1 = max_pool(self.do1, 3, 2)            \n",
    "            print('conv1 layer: ' + str(self.pool1.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv2'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.conv2 = conv2d_regularized(self.pool1, 5, 1, 64)\n",
    "            self.bn2 = tf.layers.batch_normalization(self.conv2, training = self.training)\n",
    "            self.elu2 = tf.nn.relu(self.bn2)\n",
    "            self.do2 = tf.nn.dropout(self.elu2, keep_prob = self.keep_prob)\n",
    "            self.pool2 = max_pool(self.do2, 3, 2)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('conv2 layer: ' + str(self.pool2.get_shape()))\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: Flatten the output tensor from conv2 layer                          #\n",
    "        #############################################################################\n",
    "        self.flat = flatten(self.pool2)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################      \n",
    "        print('flat layer: ' + str(self.flat.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc3'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.fc3 = fc(self.flat, 384)\n",
    "            self.bn3 = tf.layers.batch_normalization(self.fc3, training = self.training)\n",
    "            self.elu3 = tf.nn.elu(self.bn3)\n",
    "            self.do3 = tf.nn.dropout(self.elu3, keep_prob = self.keep_prob)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('fc3 layer: ' + str(self.elu3.get_shape()))\n",
    "            \n",
    "        with tf.variable_scope('fc4'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.fc4 = fc(self.do3, 10)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('fc4 layer: ' + str(self.fc4.get_shape()))\n",
    "            \n",
    "        # Return the last layer\n",
    "        return self.fc4\n",
    "        \n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        \n",
    "    def fit_zca(self, x):\n",
    "        flat_x = np.reshape(\n",
    "                x, (x.shape[0], x.shape[1] * x.shape[2] * x.shape[3]))\n",
    "        sigma = np.dot(flat_x.T, flat_x) / flat_x.shape[0]\n",
    "        u, s, _ = np.linalg.svd(sigma)\n",
    "        s_inv = 1. / np.sqrt(s[np.newaxis] + self.zca_epsilon)\n",
    "        self.principal_components = (u * s_inv).dot(u.T)\n",
    "    \n",
    "    def compute_projection(self, x):\n",
    "        flatx = np.reshape(x, (-1, np.prod(x.shape[-3:])))\n",
    "        whitex = np.dot(flatx, self.principal_components)\n",
    "        x_trans = np.reshape(whitex, x.shape)\n",
    "        return x_trans\n",
    "    \n",
    "    def train(self, sess, X_train, Y_train, X_val, Y_val):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        self.fit_zca(X_train)\n",
    "        X_train_transformed = self.compute_projection(X_train)\n",
    "        step = 0\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        reg_losses = []\n",
    "        last_time = time.time()\n",
    "        train_start = last_time\n",
    "        print('-' * 5 + '  Start training  ' + '-' * 5)\n",
    "        for epoch in range(self.num_epoch):\n",
    "            print('train for epoch %d' % epoch)\n",
    "            for i in range(num_training // self.batch_size):\n",
    "                X_ = X_train_transformed[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "                Y_ = Y_train[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                #############################################################################\n",
    "                # TODO: You can change feed data as you want                                #\n",
    "                #############################################################################\n",
    "                feed_dict = {self.X: X_, self.Y: Y_}\n",
    "                #############################################################################\n",
    "                #                             END OF YOUR CODE                              #\n",
    "                #############################################################################\n",
    "                fetches = [self.train_op, self.loss_op, self.accuracy_op, self.reg_loss_op]\n",
    "\n",
    "                _, loss, accuracy, reg_loss = sess.run(fetches, feed_dict=feed_dict)\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "                reg_losses.append(reg_loss)\n",
    "                if step % self.log_step == 0:\n",
    "                    now = time.time()\n",
    "                    print('iteration (%d): loss = %.3f, accuracy = %.3f, reg_loss: %.3f' %\n",
    "                        (step, loss, accuracy, reg_loss))\n",
    "                    print(f'Average iteration time is: {(now - last_time)/self.log_step:.3f}')\n",
    "                    print(f'Total time elapsed is {now - train_start:.3f}')\n",
    "                    \n",
    "                step += 1\n",
    "\n",
    "            # Print validation results\n",
    "            print('validation for epoch %d' % epoch)\n",
    "            start = time.time()\n",
    "            val_accuracy = self.evaluate(sess, X_val, Y_val)\n",
    "            end = time.time()\n",
    "            print('-  epoch %d: validation accuracy = %.3f' % (epoch, val_accuracy))\n",
    "            print(f'Time taken for evaluation is {end-start}')\n",
    "        \n",
    "        train_end_time = time.time()\n",
    "        print(f'Total training time is {train_end_time - train_start:.3f}')\n",
    "        \n",
    "        #############################################################################\n",
    "        # TODO: Plot training curve                                                 #\n",
    "        #############################################################################\n",
    "        # Graph 1. X: iteration (training step), Y: training loss\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.title('Training loss vs Iterations')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot(losses, '-o')\n",
    "        # Graph 2. X: iteration (training step), Y: training accuracy\n",
    "        \n",
    "        plt.subplot(2,1,2)\n",
    "        plt.title('Training accuracy vs Iterations')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.plot(accuracies, '-o')\n",
    "        \n",
    "        plt.gcf().set_size_inches(15, 15)\n",
    "        plt.show()\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    def evaluate(self, sess, X_eval, Y_eval):\n",
    "        eval_accuracy = 0.0\n",
    "        eval_iter = 0\n",
    "        for i in range(X_eval.shape[0] // self.batch_size):\n",
    "            X_ = X_eval[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "            Y_ = Y_eval[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            X_ = self.compute_projection(X_)\n",
    "            #############################################################################\n",
    "            # TODO: You can change feed data as you want                                #\n",
    "            #############################################################################\n",
    "            feed_dict = {self.X: X_, self.Y: Y_}\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            accuracy = sess.run(self.accuracy_op, feed_dict=feed_dict)\n",
    "            eval_accuracy += accuracy\n",
    "            eval_iter += 1\n",
    "        return eval_accuracy / eval_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Your model  -----\n",
      "intput layer: (?, 32, 32, 3)\n",
      "conv1 layer: (?, 16, 16, 32)\n",
      "conv2 layer: (?, 8, 8, 64)\n",
      "flat layer: (?, 4096)\n",
      "fc3 layer: (?, 384)\n",
      "fc4 layer: (?, 10)\n",
      "-----  Start training  -----\n",
      "train for epoch 0\n",
      "iteration (0): loss = 2.727, accuracy = 0.117, reg_loss: 0.037\n",
      "Average iteration time is: 0.004\n",
      "Total time elapsed is 0.426\n",
      "iteration (100): loss = 2.041, accuracy = 0.195, reg_loss: 0.036\n",
      "Average iteration time is: 0.250\n",
      "Total time elapsed is 25.019\n",
      "iteration (200): loss = 1.924, accuracy = 0.344, reg_loss: 0.035\n",
      "Average iteration time is: 0.474\n",
      "Total time elapsed is 47.373\n",
      "iteration (300): loss = 1.615, accuracy = 0.438, reg_loss: 0.034\n",
      "Average iteration time is: 0.713\n",
      "Total time elapsed is 71.323\n",
      "validation for epoch 0\n",
      "-  epoch 0: validation accuracy = 0.396\n",
      "Time taken for evaluation is 1.244807243347168\n",
      "train for epoch 1\n",
      "iteration (400): loss = 1.620, accuracy = 0.398, reg_loss: 0.033\n",
      "Average iteration time is: 0.964\n",
      "Total time elapsed is 96.398\n",
      "iteration (500): loss = 1.463, accuracy = 0.469, reg_loss: 0.033\n",
      "Average iteration time is: 1.254\n",
      "Total time elapsed is 125.382\n",
      "iteration (600): loss = 1.478, accuracy = 0.547, reg_loss: 0.032\n",
      "Average iteration time is: 1.521\n",
      "Total time elapsed is 152.077\n",
      "iteration (700): loss = 1.326, accuracy = 0.570, reg_loss: 0.032\n",
      "Average iteration time is: 1.765\n",
      "Total time elapsed is 176.526\n",
      "validation for epoch 1\n",
      "-  epoch 1: validation accuracy = 0.577\n",
      "Time taken for evaluation is 1.3192048072814941\n",
      "train for epoch 2\n",
      "iteration (800): loss = 1.229, accuracy = 0.508, reg_loss: 0.031\n",
      "Average iteration time is: 2.012\n",
      "Total time elapsed is 201.217\n",
      "iteration (900): loss = 1.295, accuracy = 0.562, reg_loss: 0.031\n",
      "Average iteration time is: 2.302\n",
      "Total time elapsed is 230.159\n",
      "iteration (1000): loss = 0.999, accuracy = 0.688, reg_loss: 0.030\n",
      "Average iteration time is: 2.540\n",
      "Total time elapsed is 253.978\n",
      "iteration (1100): loss = 1.032, accuracy = 0.719, reg_loss: 0.030\n",
      "Average iteration time is: 2.779\n",
      "Total time elapsed is 277.904\n",
      "validation for epoch 2\n",
      "-  epoch 2: validation accuracy = 0.624\n",
      "Time taken for evaluation is 1.1496782302856445\n",
      "train for epoch 3\n",
      "iteration (1200): loss = 0.910, accuracy = 0.695, reg_loss: 0.030\n",
      "Average iteration time is: 3.031\n",
      "Total time elapsed is 303.093\n",
      "iteration (1300): loss = 1.239, accuracy = 0.625, reg_loss: 0.030\n",
      "Average iteration time is: 3.269\n",
      "Total time elapsed is 326.856\n",
      "iteration (1400): loss = 0.966, accuracy = 0.711, reg_loss: 0.029\n",
      "Average iteration time is: 3.501\n",
      "Total time elapsed is 350.065\n",
      "iteration (1500): loss = 0.951, accuracy = 0.656, reg_loss: 0.030\n",
      "Average iteration time is: 3.732\n",
      "Total time elapsed is 373.249\n",
      "validation for epoch 3\n",
      "-  epoch 3: validation accuracy = 0.671\n",
      "Time taken for evaluation is 0.983055830001831\n",
      "train for epoch 4\n",
      "iteration (1600): loss = 0.833, accuracy = 0.766, reg_loss: 0.030\n",
      "Average iteration time is: 3.979\n",
      "Total time elapsed is 397.929\n",
      "iteration (1700): loss = 0.993, accuracy = 0.688, reg_loss: 0.030\n",
      "Average iteration time is: 4.222\n",
      "Total time elapsed is 422.199\n",
      "iteration (1800): loss = 0.974, accuracy = 0.648, reg_loss: 0.030\n",
      "Average iteration time is: 4.461\n",
      "Total time elapsed is 446.086\n",
      "iteration (1900): loss = 1.038, accuracy = 0.656, reg_loss: 0.030\n",
      "Average iteration time is: 4.694\n",
      "Total time elapsed is 469.351\n",
      "validation for epoch 4\n",
      "-  epoch 4: validation accuracy = 0.672\n",
      "Time taken for evaluation is 1.0880098342895508\n",
      "train for epoch 5\n",
      "iteration (2000): loss = 0.735, accuracy = 0.797, reg_loss: 0.030\n",
      "Average iteration time is: 4.955\n",
      "Total time elapsed is 495.504\n",
      "iteration (2100): loss = 1.171, accuracy = 0.664, reg_loss: 0.030\n",
      "Average iteration time is: 5.190\n",
      "Total time elapsed is 518.998\n",
      "iteration (2200): loss = 1.039, accuracy = 0.625, reg_loss: 0.030\n",
      "Average iteration time is: 5.419\n",
      "Total time elapsed is 541.891\n",
      "validation for epoch 5\n",
      "-  epoch 5: validation accuracy = 0.686\n",
      "Time taken for evaluation is 1.0088987350463867\n",
      "train for epoch 6\n",
      "iteration (2300): loss = 0.984, accuracy = 0.711, reg_loss: 0.030\n",
      "Average iteration time is: 5.667\n",
      "Total time elapsed is 566.662\n",
      "iteration (2400): loss = 0.860, accuracy = 0.758, reg_loss: 0.031\n",
      "Average iteration time is: 5.898\n",
      "Total time elapsed is 589.779\n",
      "iteration (2500): loss = 0.864, accuracy = 0.695, reg_loss: 0.031\n",
      "Average iteration time is: 6.126\n",
      "Total time elapsed is 612.647\n",
      "iteration (2600): loss = 0.774, accuracy = 0.750, reg_loss: 0.032\n",
      "Average iteration time is: 6.364\n",
      "Total time elapsed is 636.373\n",
      "validation for epoch 6\n",
      "-  epoch 6: validation accuracy = 0.694\n",
      "Time taken for evaluation is 1.0230002403259277\n",
      "train for epoch 7\n",
      "iteration (2700): loss = 0.903, accuracy = 0.695, reg_loss: 0.032\n",
      "Average iteration time is: 6.608\n",
      "Total time elapsed is 660.807\n",
      "iteration (2800): loss = 0.815, accuracy = 0.695, reg_loss: 0.032\n",
      "Average iteration time is: 6.919\n",
      "Total time elapsed is 691.913\n",
      "iteration (2900): loss = 0.739, accuracy = 0.766, reg_loss: 0.033\n",
      "Average iteration time is: 7.159\n",
      "Total time elapsed is 715.877\n",
      "iteration (3000): loss = 0.720, accuracy = 0.766, reg_loss: 0.033\n",
      "Average iteration time is: 7.403\n",
      "Total time elapsed is 740.294\n",
      "validation for epoch 7\n",
      "-  epoch 7: validation accuracy = 0.708\n",
      "Time taken for evaluation is 1.0922183990478516\n",
      "train for epoch 8\n",
      "iteration (3100): loss = 0.712, accuracy = 0.750, reg_loss: 0.034\n",
      "Average iteration time is: 7.663\n",
      "Total time elapsed is 766.290\n",
      "iteration (3200): loss = 0.892, accuracy = 0.711, reg_loss: 0.034\n",
      "Average iteration time is: 7.962\n",
      "Total time elapsed is 796.168\n",
      "iteration (3300): loss = 0.614, accuracy = 0.781, reg_loss: 0.034\n",
      "Average iteration time is: 8.223\n",
      "Total time elapsed is 822.290\n",
      "iteration (3400): loss = 0.674, accuracy = 0.781, reg_loss: 0.035\n",
      "Average iteration time is: 8.485\n",
      "Total time elapsed is 848.549\n",
      "validation for epoch 8\n",
      "-  epoch 8: validation accuracy = 0.731\n",
      "Time taken for evaluation is 1.0763764381408691\n",
      "train for epoch 9\n",
      "iteration (3500): loss = 0.877, accuracy = 0.672, reg_loss: 0.035\n",
      "Average iteration time is: 8.756\n",
      "Total time elapsed is 875.634\n",
      "iteration (3600): loss = 0.674, accuracy = 0.781, reg_loss: 0.036\n",
      "Average iteration time is: 9.024\n",
      "Total time elapsed is 902.409\n",
      "iteration (3700): loss = 0.938, accuracy = 0.688, reg_loss: 0.036\n",
      "Average iteration time is: 9.292\n",
      "Total time elapsed is 929.224\n",
      "iteration (3800): loss = 0.717, accuracy = 0.719, reg_loss: 0.037\n",
      "Average iteration time is: 9.560\n",
      "Total time elapsed is 955.993\n",
      "validation for epoch 9\n",
      "-  epoch 9: validation accuracy = 0.723\n",
      "Time taken for evaluation is 1.3130912780761719\n",
      "train for epoch 10\n",
      "iteration (3900): loss = 0.672, accuracy = 0.789, reg_loss: 0.038\n",
      "Average iteration time is: 9.859\n",
      "Total time elapsed is 985.856\n",
      "iteration (4000): loss = 0.638, accuracy = 0.789, reg_loss: 0.038\n",
      "Average iteration time is: 10.141\n",
      "Total time elapsed is 1014.055\n",
      "iteration (4100): loss = 0.635, accuracy = 0.812, reg_loss: 0.039\n",
      "Average iteration time is: 10.399\n",
      "Total time elapsed is 1039.922\n",
      "iteration (4200): loss = 0.642, accuracy = 0.805, reg_loss: 0.040\n",
      "Average iteration time is: 10.679\n",
      "Total time elapsed is 1067.945\n",
      "validation for epoch 10\n",
      "-  epoch 10: validation accuracy = 0.741\n",
      "Time taken for evaluation is 1.4042158126831055\n",
      "train for epoch 11\n",
      "iteration (4300): loss = 0.872, accuracy = 0.750, reg_loss: 0.040\n",
      "Average iteration time is: 11.027\n",
      "Total time elapsed is 1102.719\n",
      "iteration (4400): loss = 0.699, accuracy = 0.750, reg_loss: 0.041\n",
      "Average iteration time is: 11.348\n",
      "Total time elapsed is 1134.850\n",
      "iteration (4500): loss = 0.687, accuracy = 0.766, reg_loss: 0.042\n",
      "Average iteration time is: 11.645\n",
      "Total time elapsed is 1164.489\n",
      "validation for epoch 11\n",
      "-  epoch 11: validation accuracy = 0.743\n",
      "Time taken for evaluation is 1.296706199645996\n",
      "train for epoch 12\n",
      "iteration (4600): loss = 0.551, accuracy = 0.828, reg_loss: 0.042\n",
      "Average iteration time is: 11.950\n",
      "Total time elapsed is 1194.958\n",
      "iteration (4700): loss = 0.709, accuracy = 0.766, reg_loss: 0.043\n",
      "Average iteration time is: 12.220\n",
      "Total time elapsed is 1221.978\n",
      "iteration (4800): loss = 0.561, accuracy = 0.820, reg_loss: 0.043\n",
      "Average iteration time is: 12.509\n",
      "Total time elapsed is 1250.932\n",
      "iteration (4900): loss = 0.612, accuracy = 0.797, reg_loss: 0.044\n",
      "Average iteration time is: 12.843\n",
      "Total time elapsed is 1284.281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation for epoch 12\n",
      "-  epoch 12: validation accuracy = 0.740\n",
      "Time taken for evaluation is 1.3539912700653076\n",
      "train for epoch 13\n",
      "iteration (5000): loss = 0.489, accuracy = 0.852, reg_loss: 0.045\n",
      "Average iteration time is: 13.172\n",
      "Total time elapsed is 1317.199\n",
      "iteration (5100): loss = 0.605, accuracy = 0.789, reg_loss: 0.045\n",
      "Average iteration time is: 13.460\n",
      "Total time elapsed is 1346.041\n",
      "iteration (5200): loss = 0.502, accuracy = 0.828, reg_loss: 0.046\n",
      "Average iteration time is: 13.749\n",
      "Total time elapsed is 1374.862\n",
      "iteration (5300): loss = 0.657, accuracy = 0.766, reg_loss: 0.047\n",
      "Average iteration time is: 14.042\n",
      "Total time elapsed is 1404.176\n",
      "validation for epoch 13\n",
      "-  epoch 13: validation accuracy = 0.737\n",
      "Time taken for evaluation is 1.31243896484375\n",
      "train for epoch 14\n",
      "iteration (5400): loss = 0.660, accuracy = 0.758, reg_loss: 0.048\n",
      "Average iteration time is: 14.348\n",
      "Total time elapsed is 1434.824\n",
      "iteration (5500): loss = 0.677, accuracy = 0.789, reg_loss: 0.048\n",
      "Average iteration time is: 14.639\n",
      "Total time elapsed is 1463.905\n",
      "iteration (5600): loss = 0.544, accuracy = 0.844, reg_loss: 0.049\n",
      "Average iteration time is: 14.933\n",
      "Total time elapsed is 1493.272\n",
      "iteration (5700): loss = 0.626, accuracy = 0.812, reg_loss: 0.049\n",
      "Average iteration time is: 15.278\n",
      "Total time elapsed is 1527.768\n",
      "validation for epoch 14\n",
      "-  epoch 14: validation accuracy = 0.740\n",
      "Time taken for evaluation is 1.7325966358184814\n",
      "train for epoch 15\n",
      "iteration (5800): loss = 0.594, accuracy = 0.812, reg_loss: 0.050\n",
      "Average iteration time is: 15.631\n",
      "Total time elapsed is 1563.138\n",
      "iteration (5900): loss = 0.527, accuracy = 0.812, reg_loss: 0.050\n",
      "Average iteration time is: 15.900\n",
      "Total time elapsed is 1590.039\n",
      "iteration (6000): loss = 0.669, accuracy = 0.742, reg_loss: 0.051\n",
      "Average iteration time is: 16.170\n",
      "Total time elapsed is 1616.990\n",
      "iteration (6100): loss = 0.554, accuracy = 0.805, reg_loss: 0.052\n",
      "Average iteration time is: 16.479\n",
      "Total time elapsed is 1647.931\n",
      "validation for epoch 15\n",
      "-  epoch 15: validation accuracy = 0.737\n",
      "Time taken for evaluation is 1.4222517013549805\n",
      "train for epoch 16\n",
      "iteration (6200): loss = 0.571, accuracy = 0.781, reg_loss: 0.052\n",
      "Average iteration time is: 16.765\n",
      "Total time elapsed is 1676.538\n",
      "iteration (6300): loss = 0.464, accuracy = 0.867, reg_loss: 0.053\n",
      "Average iteration time is: 17.032\n",
      "Total time elapsed is 1703.160\n",
      "iteration (6400): loss = 0.755, accuracy = 0.797, reg_loss: 0.053\n",
      "Average iteration time is: 17.277\n",
      "Total time elapsed is 1727.726\n",
      "validation for epoch 16\n",
      "-  epoch 16: validation accuracy = 0.742\n",
      "Time taken for evaluation is 1.1045465469360352\n",
      "train for epoch 17\n",
      "iteration (6500): loss = 0.645, accuracy = 0.781, reg_loss: 0.054\n",
      "Average iteration time is: 17.536\n",
      "Total time elapsed is 1753.646\n",
      "iteration (6600): loss = 0.503, accuracy = 0.820, reg_loss: 0.054\n",
      "Average iteration time is: 17.835\n",
      "Total time elapsed is 1783.491\n",
      "iteration (6700): loss = 0.518, accuracy = 0.844, reg_loss: 0.055\n",
      "Average iteration time is: 18.074\n",
      "Total time elapsed is 1807.422\n",
      "iteration (6800): loss = 0.539, accuracy = 0.797, reg_loss: 0.055\n",
      "Average iteration time is: 18.310\n",
      "Total time elapsed is 1830.966\n",
      "validation for epoch 17\n",
      "-  epoch 17: validation accuracy = 0.723\n",
      "Time taken for evaluation is 1.0453283786773682\n",
      "train for epoch 18\n",
      "iteration (6900): loss = 0.561, accuracy = 0.797, reg_loss: 0.056\n",
      "Average iteration time is: 18.556\n",
      "Total time elapsed is 1855.623\n",
      "iteration (7000): loss = 0.635, accuracy = 0.797, reg_loss: 0.056\n",
      "Average iteration time is: 18.792\n",
      "Total time elapsed is 1879.240\n",
      "iteration (7100): loss = 0.537, accuracy = 0.805, reg_loss: 0.057\n",
      "Average iteration time is: 19.051\n",
      "Total time elapsed is 1905.076\n",
      "iteration (7200): loss = 0.570, accuracy = 0.781, reg_loss: 0.058\n",
      "Average iteration time is: 19.288\n",
      "Total time elapsed is 1928.794\n",
      "validation for epoch 18\n",
      "-  epoch 18: validation accuracy = 0.747\n",
      "Time taken for evaluation is 1.0749621391296387\n",
      "train for epoch 19\n",
      "iteration (7300): loss = 0.565, accuracy = 0.805, reg_loss: 0.058\n",
      "Average iteration time is: 19.537\n",
      "Total time elapsed is 1953.661\n",
      "iteration (7400): loss = 0.611, accuracy = 0.820, reg_loss: 0.058\n",
      "Average iteration time is: 19.777\n",
      "Total time elapsed is 1977.711\n",
      "iteration (7500): loss = 0.438, accuracy = 0.867, reg_loss: 0.059\n",
      "Average iteration time is: 20.034\n",
      "Total time elapsed is 2003.353\n",
      "iteration (7600): loss = 0.472, accuracy = 0.867, reg_loss: 0.059\n",
      "Average iteration time is: 20.316\n",
      "Total time elapsed is 2031.579\n",
      "validation for epoch 19\n",
      "-  epoch 19: validation accuracy = 0.729\n",
      "Time taken for evaluation is 1.0854547023773193\n",
      "train for epoch 20\n",
      "iteration (7700): loss = 0.424, accuracy = 0.852, reg_loss: 0.060\n",
      "Average iteration time is: 20.596\n",
      "Total time elapsed is 2059.590\n",
      "iteration (7800): loss = 0.472, accuracy = 0.852, reg_loss: 0.060\n",
      "Average iteration time is: 20.847\n",
      "Total time elapsed is 2084.674\n",
      "iteration (7900): loss = 0.481, accuracy = 0.875, reg_loss: 0.061\n",
      "Average iteration time is: 21.148\n",
      "Total time elapsed is 2114.842\n",
      "iteration (8000): loss = 0.442, accuracy = 0.867, reg_loss: 0.061\n",
      "Average iteration time is: 21.389\n",
      "Total time elapsed is 2138.885\n",
      "validation for epoch 20\n",
      "-  epoch 20: validation accuracy = 0.734\n",
      "Time taken for evaluation is 1.0894460678100586\n",
      "train for epoch 21\n",
      "iteration (8100): loss = 0.445, accuracy = 0.867, reg_loss: 0.062\n",
      "Average iteration time is: 21.654\n",
      "Total time elapsed is 2165.398\n",
      "iteration (8200): loss = 0.414, accuracy = 0.859, reg_loss: 0.062\n",
      "Average iteration time is: 21.891\n",
      "Total time elapsed is 2189.147\n",
      "iteration (8300): loss = 0.533, accuracy = 0.805, reg_loss: 0.062\n",
      "Average iteration time is: 22.145\n",
      "Total time elapsed is 2214.498\n",
      "iteration (8400): loss = 0.437, accuracy = 0.859, reg_loss: 0.063\n",
      "Average iteration time is: 22.414\n",
      "Total time elapsed is 2241.434\n",
      "validation for epoch 21\n",
      "-  epoch 21: validation accuracy = 0.748\n",
      "Time taken for evaluation is 1.1187520027160645\n",
      "train for epoch 22\n",
      "iteration (8500): loss = 0.418, accuracy = 0.844, reg_loss: 0.063\n",
      "Average iteration time is: 22.678\n",
      "Total time elapsed is 2267.830\n",
      "iteration (8600): loss = 0.491, accuracy = 0.859, reg_loss: 0.064\n",
      "Average iteration time is: 22.941\n",
      "Total time elapsed is 2294.142\n",
      "iteration (8700): loss = 0.535, accuracy = 0.828, reg_loss: 0.064\n",
      "Average iteration time is: 23.219\n",
      "Total time elapsed is 2321.869\n",
      "validation for epoch 22\n",
      "-  epoch 22: validation accuracy = 0.734\n",
      "Time taken for evaluation is 1.113434076309204\n",
      "train for epoch 23\n",
      "iteration (8800): loss = 0.366, accuracy = 0.891, reg_loss: 0.064\n",
      "Average iteration time is: 23.494\n",
      "Total time elapsed is 2349.374\n",
      "iteration (8900): loss = 0.491, accuracy = 0.844, reg_loss: 0.065\n",
      "Average iteration time is: 23.741\n",
      "Total time elapsed is 2374.111\n",
      "iteration (9000): loss = 0.525, accuracy = 0.828, reg_loss: 0.065\n",
      "Average iteration time is: 23.998\n",
      "Total time elapsed is 2399.754\n",
      "iteration (9100): loss = 0.368, accuracy = 0.898, reg_loss: 0.065\n",
      "Average iteration time is: 24.280\n",
      "Total time elapsed is 2428.039\n",
      "validation for epoch 23\n",
      "-  epoch 23: validation accuracy = 0.747\n",
      "Time taken for evaluation is 1.061638355255127\n",
      "train for epoch 24\n",
      "iteration (9200): loss = 0.428, accuracy = 0.852, reg_loss: 0.066\n",
      "Average iteration time is: 24.549\n",
      "Total time elapsed is 2454.943\n",
      "iteration (9300): loss = 0.469, accuracy = 0.867, reg_loss: 0.066\n",
      "Average iteration time is: 24.798\n",
      "Total time elapsed is 2479.818\n",
      "iteration (9400): loss = 0.354, accuracy = 0.898, reg_loss: 0.066\n",
      "Average iteration time is: 25.114\n",
      "Total time elapsed is 2511.354\n",
      "iteration (9500): loss = 0.483, accuracy = 0.828, reg_loss: 0.067\n",
      "Average iteration time is: 25.373\n",
      "Total time elapsed is 2537.323\n",
      "validation for epoch 24\n",
      "-  epoch 24: validation accuracy = 0.749\n",
      "Time taken for evaluation is 1.304044485092163\n",
      "train for epoch 25\n",
      "iteration (9600): loss = 0.493, accuracy = 0.875, reg_loss: 0.067\n",
      "Average iteration time is: 25.698\n",
      "Total time elapsed is 2569.801\n",
      "iteration (9700): loss = 0.439, accuracy = 0.867, reg_loss: 0.067\n",
      "Average iteration time is: 25.954\n",
      "Total time elapsed is 2595.444\n",
      "iteration (9800): loss = 0.440, accuracy = 0.867, reg_loss: 0.067\n",
      "Average iteration time is: 26.242\n",
      "Total time elapsed is 2624.215\n",
      "iteration (9900): loss = 0.353, accuracy = 0.898, reg_loss: 0.068\n",
      "Average iteration time is: 26.505\n",
      "Total time elapsed is 2650.534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation for epoch 25\n",
      "-  epoch 25: validation accuracy = 0.737\n",
      "Time taken for evaluation is 1.1729979515075684\n",
      "train for epoch 26\n",
      "iteration (10000): loss = 0.308, accuracy = 0.914, reg_loss: 0.068\n",
      "Average iteration time is: 26.779\n",
      "Total time elapsed is 2677.930\n",
      "iteration (10100): loss = 0.377, accuracy = 0.898, reg_loss: 0.068\n",
      "Average iteration time is: 27.062\n",
      "Total time elapsed is 2706.205\n",
      "iteration (10200): loss = 0.302, accuracy = 0.906, reg_loss: 0.068\n",
      "Average iteration time is: 27.339\n",
      "Total time elapsed is 2733.930\n",
      "iteration (10300): loss = 0.457, accuracy = 0.844, reg_loss: 0.068\n",
      "Average iteration time is: 27.627\n",
      "Total time elapsed is 2762.680\n",
      "validation for epoch 26\n",
      "-  epoch 26: validation accuracy = 0.766\n",
      "Time taken for evaluation is 1.2530317306518555\n",
      "train for epoch 27\n",
      "iteration (10400): loss = 0.421, accuracy = 0.891, reg_loss: 0.069\n",
      "Average iteration time is: 27.991\n",
      "Total time elapsed is 2799.150\n",
      "iteration (10500): loss = 0.346, accuracy = 0.906, reg_loss: 0.069\n",
      "Average iteration time is: 28.328\n",
      "Total time elapsed is 2832.765\n",
      "iteration (10600): loss = 0.458, accuracy = 0.867, reg_loss: 0.069\n",
      "Average iteration time is: 28.604\n",
      "Total time elapsed is 2860.443\n",
      "validation for epoch 27\n",
      "-  epoch 27: validation accuracy = 0.740\n",
      "Time taken for evaluation is 1.158689260482788\n",
      "train for epoch 28\n",
      "iteration (10700): loss = 0.413, accuracy = 0.875, reg_loss: 0.069\n",
      "Average iteration time is: 28.923\n",
      "Total time elapsed is 2892.317\n",
      "iteration (10800): loss = 0.344, accuracy = 0.891, reg_loss: 0.069\n",
      "Average iteration time is: 29.270\n",
      "Total time elapsed is 2926.954\n",
      "iteration (10900): loss = 0.274, accuracy = 0.930, reg_loss: 0.070\n",
      "Average iteration time is: 29.655\n",
      "Total time elapsed is 2965.532\n",
      "iteration (11000): loss = 0.321, accuracy = 0.906, reg_loss: 0.070\n",
      "Average iteration time is: 29.947\n",
      "Total time elapsed is 2994.705\n",
      "validation for epoch 28\n",
      "-  epoch 28: validation accuracy = 0.756\n",
      "Time taken for evaluation is 1.6517534255981445\n",
      "train for epoch 29\n",
      "iteration (11100): loss = 0.345, accuracy = 0.906, reg_loss: 0.070\n",
      "Average iteration time is: 30.285\n",
      "Total time elapsed is 3028.529\n",
      "iteration (11200): loss = 0.443, accuracy = 0.859, reg_loss: 0.070\n",
      "Average iteration time is: 30.592\n",
      "Total time elapsed is 3059.212\n",
      "iteration (11300): loss = 0.471, accuracy = 0.844, reg_loss: 0.070\n",
      "Average iteration time is: 30.901\n",
      "Total time elapsed is 3090.093\n",
      "iteration (11400): loss = 0.506, accuracy = 0.859, reg_loss: 0.071\n",
      "Average iteration time is: 31.229\n",
      "Total time elapsed is 3122.922\n",
      "validation for epoch 29\n",
      "-  epoch 29: validation accuracy = 0.757\n",
      "Time taken for evaluation is 1.344024896621704\n",
      "train for epoch 30\n",
      "iteration (11500): loss = 0.372, accuracy = 0.883, reg_loss: 0.071\n",
      "Average iteration time is: 31.553\n",
      "Total time elapsed is 3155.325\n",
      "iteration (11600): loss = 0.386, accuracy = 0.875, reg_loss: 0.071\n",
      "Average iteration time is: 31.905\n",
      "Total time elapsed is 3190.505\n"
     ]
    }
   ],
   "source": [
    "# Clear old computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    with tf.device('/cpu:0'):\n",
    "        model = YourModel()\n",
    "        model.train(sess, X_train, Y_train, X_val, Y_val)\n",
    "        accuracy = model.evaluate(sess, X_test, Y_test)\n",
    "        print('***** test accuracy: %.3f' % accuracy)\n",
    "        # Save your model\n",
    "        saver = tf.train.Saver()\n",
    "        model_path = saver.save(sess, \"lib/tf_models/problem2/csci-599_mine.ckpt\")\n",
    "        print(\"Model saved in %s\" % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model specifications\n",
    "1. With BN: test acc: 0.61\n",
    "2. with BN and dropout: 0.644"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Load your model\n",
    "model = YourModel()\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"lib/tf_models/problem2/csci-599_mine.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
